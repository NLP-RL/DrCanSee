# DrCanSee

The repository will have the code and data for our research work titled 'Dr Can See: Towards Multimodal Automatic Disease Diagnosis'. 

The paper has been accepted at CIKM2022 (31st ACM International Conference on Information and Knowledge Management)

## Abstract: 

Artificial Intelligence-based clinical decision support is gaining ever-growing popularity and demand in both the research and industry communities. One such manifestation is automatic disease diagnosis, which aims to assist clinicians in conducting symptom investigation and disease diagnosis. When we consult with doctors, we often report and describe our health conditions with visual aids. Moreover, many people are unacquainted with several symptoms and medical terms, such as mouth ulcer and skin growth. Therefore, visual form of symptom reporting is more of a necessity. However, existing disease diagnosis assistants fail to consider signs/symptoms from visual aid, and they rely solely on symptom information extracted through text responses. Motivated by the efficacy of visual form of symptom reporting, we propose and build a novel end-to-end Multimodal Disease Diagnosis Virtual Assistant (MDD-VA) using reinforcement learning technique. In conversation, users' responses are heavily influenced by the ongoing dialogue context, and multimodal responses appear to be of no difference. We also propose and incorporate a Context-aware Symptom Image Identification module that leverages discourse context in addition to the symptom image for identifying symptoms effectively. Furthermore, we first curate a multimodal conversational medical dialogue corpus in English that is annotated with intent, symptoms, and visual information. The proposed MDD-VA outperforms multiple uni-modal baselines in both automatic and human evaluation, which firmly establishes the critical role of symptom information provided by visuals.

